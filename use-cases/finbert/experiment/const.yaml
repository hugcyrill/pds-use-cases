name: FinBertPyTorch_Const
hyperparameters:
    max_seq_length: 64
    global_batch_size: 64
    learning_rate: 2.0e-5
    lr_scheduler_epoch_freq: 1
    model_type: 'bert_for_classification'
    adam_epsilon: 1.0e-8
    weight_decay: 0
    num_warmup_steps: 0
    doc_stride: 1
    n_best_size: 20
    null_score_diff_threshold: 0.0
    max_grad_norm: 1.0
    num_training_steps: 218 # This is the number of optimizer steps. Set it
                              # to max_length.batches (assuming we step every
                              # batch in the LR scheduler definition)
records_per_epoch: 3488
searcher:
    name: single
    metric: validation_loss 
    max_length:
        epochs: 4           # There are 3488k examples in the training set and 388 examples in the validation set.   
    smaller_is_better: true
min_validation_period:
    epochs: 1               # Validation after each epoch           
data:
    pretrained_model_name: "bert-base-uncased"
    download_data: False
    task: "classification"
    local_path: ./data/sentiment_data
entrypoint: model_def:FinBERTPyTorch